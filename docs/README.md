# VulkanIlm ðŸš€ðŸ”¥ â€“ GPU-Accelerated Local LLMs for Everyone

![GitHub stars](https://img.shields.io/github/stars/ggerganov/llama.cpp?style=social)
![GitHub forks](https://img.shields.io/github/forks/ggerganov/llama.cpp?style=social)
![GitHub license](https://img.shields.io/github/license/Talnz007/VulkanIlm)
![Python version](https://img.shields.io/badge/python-3.9+-blue.svg)
![GitHub issues](https://img.shields.io/github/issues/Talnz007/VulkanIlm)

**VulkanIlm** (from _"Vulkan"_ ðŸ”¥ + _"Ilm"_ ðŸ“š, meaning "knowledge" in Urdu/Arabic) is a Python library that brings GPU-accelerated local LLM inference to **AMD and Intel GPU users**â€”no CUDA required.

Built specifically for developers with legacy GPUs, VulkanIlm enables blazing-fast local inference using [`llama.cpp`](https://github.com/ggerganov/llama.cpp)'s Vulkan backend.

---

## ðŸŽ¯ Why VulkanIlm?

> **Not everyone has an NVIDIA GPU. Everyone deserves fast local AI.**

**The Problem:** Most GPU acceleration libraries focus on CUDA, leaving AMD/Intel users with slow CPU-only inference.

**The Solution:** VulkanIlm democratizes GPU-accelerated AI for **all GPU brands** using Vulkan.

### Key Features

- ðŸš€ **4-6x faster** than CPU inference on legacy GPUs
- ðŸŽ® **Universal GPU support**: AMD, Intel, and NVIDIA
- ðŸ **Python-first** with simple CLI tools
- âš¡ **Auto-detection** and optimization for your GPU
- ðŸ“¦ **Zero-config installation** with auto-building
- ðŸ”„ **Real-time streaming** token generation

---

## ðŸ“Š Performance Results

| Hardware | CPU Performance | Vulkan Performance | **Speedup** |
|----------|-----------------|-------------------|-------------|
| AMD RX 580 8GB | 188.47s | 44.74s | **4.21x** |
| Intel Arc A770 | ~120s | ~25s | **4.8x** |
| AMD RX 6600 | ~90s | ~18s | **5.0x** |

*Benchmarked with Gemma-3n-E4B-it model (6.9B parameters)*

---

## ðŸ“¦ Installation

### Quick Start

```bash
git clone https://github.com/Talnz007/VulkanIlm.git
cd VulkanIlm
pip install -e .
```

### Prerequisites

- **Python 3.9+**
- **Vulkan-capable GPU** (AMD RX 400+, Intel Arc/Xe, NVIDIA GTX 900+)
- **Vulkan drivers** installed

### Install Vulkan Drivers (if needed)

```bash
# Ubuntu/Debian
sudo apt install vulkan-tools libvulkan-dev

# Fedora/RHEL
sudo dnf install vulkan-tools vulkan-devel

# Test installation
vulkaninfo
```

---

## ðŸš€ Usage

### CLI Interface

```bash
# Auto-install llama.cpp with Vulkan support
vulkanilm install

# Check your GPU setup
vulkanilm vulkan-info

# Download and use models
vulkanilm search "llama"
vulkanilm download microsoft/DialoGPT-medium

# Generate text
vulkanilm ask model.gguf --prompt "Explain quantum computing"

# Stream in real-time
vulkanilm stream model.gguf "Tell me a story about AI"

# Benchmark CPU vs GPU performance
vulkanilm benchmark "Test prompt"
```

### Python API

```python
from vulkan_ilm import Llama

# Load model with automatic GPU optimization
llm = Llama("path/to/model.gguf", gpu_layers=16)

# Generate text
response = llm.ask("Explain the term 'ilm' in AI context.")
print(response)

# Stream tokens in real-time
for token in llm.stream_ask_real("Tell me about Vulkan API"):
    print(token, end='', flush=True)
```

---

## ðŸŽ® Supported Hardware

### Tested GPUs

| Brand | Models | Status | Performance |
|-------|--------|--------|-------------|
| **AMD** | RX 580, RX 590, RX 6600, RX 6700 | âœ… Excellent | 4-5x speedup |
| **Intel** | Arc A770, Arc A750, Xe Graphics | âœ… Great | 4-5x speedup |
| **NVIDIA** | GTX 1060+, RTX series | âœ… Excellent | 4-6x speedup |

---

## ðŸ› ï¸ Troubleshooting

### Common Issues

**âŒ `vulkanilm: command not found`**

```bash
# Ensure you installed the package correctly
pip install -e .
poetry install  # if using Poetry

# Check if CLI is available
which vulkanilm
```

**âŒ `No Vulkan support detected`**

```bash
# Install Vulkan tools and test
sudo apt install vulkan-tools libvulkan-dev
vulkaninfo

# Update GPU drivers
```

**âŒ `Model hangs or uses too much VRAM`**

```bash
# Use fewer GPU layers
vulkanilm ask model.gguf --gpu-layers 8 -p "test"

# Or force CPU mode for testing
vulkanilm ask model.gguf --cpu -p "test"
```

---

## ðŸ—ï¸ Architecture

```
VulkanIlm/
â”œâ”€â”€ vulkan_ilm/
â”‚   â”œâ”€â”€ cli.py              # Command-line interface
â”‚   â”œâ”€â”€ llama.py            # Main Python API
â”‚   â”œâ”€â”€ vulkan/
â”‚   â”‚   â””â”€â”€ detector.py     # GPU detection & optimization
â”‚   â”œâ”€â”€ benchmark.py        # Performance testing
â”‚   â”œâ”€â”€ installer.py        # Auto-build llama.cpp
â”‚   â””â”€â”€ streaming.py        # Real-time token streaming
â”œâ”€â”€ pyproject.toml          # Poetry configuration
â””â”€â”€ README.md
```

---

## ðŸ¤ Contributing

We welcome contributions! Areas where you can help:

- **GPU Testing**: Test on different AMD/Intel/NVIDIA cards
- **Model Support**: Add support for new model formats
- **Performance**: Optimize memory usage and speed
- **Documentation**: Improve guides and examples

See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

---

## ðŸ§¾ The Story Behind "VulkanIlm"

In South Asian and Islamic culture, **"Ilm" (Ø¹Ù„Ù…)** represents *knowledge*, wisdom, and enlightenment.

Combined with **Vulkan**â€”a high-performance GPU APIâ€”this project embodies **"knowledge on fire"** ðŸ”¥: making advanced AI accessible to everyone, regardless of their GPU brand or budget.

**Our mission:** Democratize local AI inference for the global developer community.

---

## ðŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ðŸ“ž Support & Links

- **GitHub**: [https://github.com/Talnz007/VulkanIlm](https://github.com/Talnz007/VulkanIlm)
- **Issues**: [Report bugs or request features](https://github.com/Talnz007/VulkanIlm/issues)
- **Discussions**: [Community Q&A](https://github.com/Talnz007/VulkanIlm/discussions)

---

> **ðŸ”¥ Built with passion by [@Talnz007](https://github.com/Talnz007) â€” Bringing fast, local AI to legacy GPUs everywhere**
